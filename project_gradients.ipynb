{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project: Gradients and Solvers\n",
    "\n",
    "## Machine Learning, Fall 2021\n",
    "\n",
    "### Name: [Your name here]\n",
    "\n",
    "### Sources\n",
    "* _Add any and all sources here, including peers with whom your worked, websites, and other resources used._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this project, you'll perform the following tasks:\n",
    "\n",
    "1. [Explore gradients and contour plots](#Gradients-and-Contour-Plots),\n",
    "2. [Learn about the notion of a \"Blackbox\" algorithm](#Learning-about-the-black-box)\n",
    "2. [Define a `Variable` class](#The-Variable-class) which is used to compute the gradient of an \"arbitrary\" function,\n",
    "3. [Test out your `Variable` class](#A-basic-test-of-your-Variable-class) by plotting some more gradients on contour plots, \n",
    "4. [Build and test a `LogisticRegression` class](#The-LogisticRegression-class) using your `Variable` class, and \n",
    "6. [Explore your fit model to ask questions about the black box](#Exploring-the-black-box)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Standard import statements:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradients and Contour Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's begin by understanding how contour plots work.  These are 2D representations of functions of two variables (what some people might call \"3D functions\").  You may be familiar with [contour maps](http://sitesmedia.s3.amazonaws.com/creekconnections/files/2014/09/topomap.jpg); a contour map is simply a contour plot of the function \n",
    "\n",
    "$$f(\\text{lattitude}, \\text{longitude}) = \\left<\\text{height of the Earth's surface above sea level at that point}\\right>.$$ \n",
    "\n",
    "More generally, the contours in a plot are the paths of same height.  Thus, traveling perpendicularly to a contour means traveling \"straight up or down the mountain\".  Here's a first attempt at making a contour plot for a function which somewhat resembles the top of a mountain:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def f(x,y):\n",
    "    return 1.2 - 0.2*x**2 - 0.3*y**2 + 0.1 * x * y - 0.25 * x\n",
    "\n",
    "x = np.arange(-8,8,.1)\n",
    "y = np.arange(-8,8,.1)\n",
    "\n",
    "# maybe a first guess is:\n",
    "z = f(x,y)\n",
    "\n",
    "try:\n",
    "    plt.contour(x, y, z);\n",
    "except Exception as e:\n",
    "    print(\"Exception was raised: \\n\", type(e).__name__, \": \", e, sep='')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That doesn't work, because this only plugs values into $f$ which are on the *diagonal*, that is, those values in the $xy$-plane where $x=y$.  In order to make this work, we need a function value over every point in our grid, i.e. every pair of $x$- and $y$-values in the square.  This is why the value for z must be 2D, because its indices `i` and `j` are the indices of its $x$- and $y$-values of its point.  That is, `z[i,j] = f(x[i], y[j])`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Better, but super confusing. Try to parse this line:\n",
    "z = np.array([[f(x[i],y[j]) for i in range(x.shape[0])] for j in range(y.shape[0])])\n",
    "\n",
    "try:\n",
    "    plt.contour(x, y, z, cmap='Spectral');\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That works, but that line defining `z` sure was a slog.  This is where the helper function `np.meshgrid` comes into play:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X,Y)\n",
    "\n",
    "# Convenience for printing \n",
    "iters = [('x', x), ('X', X), ('y', y), ('Y', Y), ('z', z) , ('Z', Z)]\n",
    "\n",
    "for name, var in iters:\n",
    "    print(\"The shape of {} is {}\".format(name, var.shape))\n",
    "\n",
    "if (z == Z).all():\n",
    "    print(\"\\nThese ways work the same\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So `np.meshgrid` is used to build this `Z` (which is the same as our manually-built `z`).  But what are `X` and `Y`?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Make sure you understand what the following code is doing!  It will help with understanding X and Y.\n",
    "\n",
    "for name, var in iters[:4]:\n",
    "    try:\n",
    "        snipped = var[:6, :6]\n",
    "    except:\n",
    "        snipped = var[:6]\n",
    "    print('Beginning of {}:\\n{}'.format(name, snipped), end='\\n\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"background-color: #EFDDFF; padding: 10px\">\n",
    "<p>Describe what you're seeing:</p>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_(Your response goes here; delete this text and replace with your response.)_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, so now that we know how to plot contour plots of functions of 2 variables, let's explore adding some **gradients** to the plot.  First we define a helper function, used to help us draw the arrows on contour plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def draw_arrow(a, b, c, d, ax=None):\n",
    "    \"\"\" Draw an arrow on a plot.\n",
    "    \n",
    "    params:\n",
    "        a, b -- coordinates of the base of the arrow\n",
    "        c, d -- vector corresponding to the arrow (starting at the origin)\n",
    "        ax   -- the matplotlib Axes object on which to draw the arrow.  If none, set to current Axes object.\n",
    "    \"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    v_0 = np.array([a,b])\n",
    "    v_1 = np.array([c,d]) + v_0\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    color='k')\n",
    "    plt.annotate('', v_1, v_0, arrowprops=arrowprops)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot the contour plot as well as some gradient vectors\n",
    "x = np.arange(-8,8,.1)\n",
    "y = np.arange(-8,8,.1)\n",
    "\n",
    "X, Y = np.meshgrid(x,y)\n",
    "Z = f(X,Y)\n",
    "\n",
    "plt.contour(x, y, Z, cmap='winter')\n",
    "\n",
    "# Draw some gradients\n",
    "draw_arrow(2, -4, -1.45,  2.6)\n",
    "draw_arrow(-2, -2, 0.35, 1)\n",
    "draw_arrow(4, 4, -1.45, -2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"background-color: #EFDDFF; padding: 10px;\">\n",
    "<p> Okay, so now here's your task. The gradient of the above function happens to be:\n",
    "$$\\nabla f(x,y) = \\left<-.4 x + .1 y - .25, -.6 y + .1 x\\right>.$$ \n",
    "\n",
    "What this means is: at any point $(x, y)$, the gradient is a vector whose $x$- and $y$-components are given by the right-hand side of the equation. You don't need to know how to compute that gradient.\n",
    "    \n",
    "Create a function `draw_gradient` that, given a point $(x,y)$, draws the gradient of $f$ at that point.  Then, in the same cell below, recreate the above plot so that your `draw_gradient` method gets called for a handful of randomly-generated points (so that you can rerun the cell and get a new handful of gradient vectors drawn each time).</p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Good luck!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"background-color: #EFDFDF; padding: 10px;\">\n",
    "\n",
    "<h3> Rules for Gradients:</h3>\n",
    "\n",
    "<p>As we have seen, the gradient is a function that points us \"up the hill\", in the direction of steepest increase of a function $f$.  What does that mean?  It's important to keep in mind that if you have a function $f$ then the gradient of $f$, written as $\\nabla f$, is a different function that takes in points from the same space $f$ does and outputs a vector which points up toward the direction of steepest increase at that point.</p>\n",
    "\n",
    "<p>Below are all the rules that we will need for our gradients.  In a multivariable calculus class, you would study and possibly even prove these statements.  For us, we'll take them as our programming requirements: the gradient is simply a recursive function with a bunch of rules.  <strong>Do not feel like you need to memorize these for my class, or understand where they come from. I will never ask you to compute the gradient of a function by hand.</strong></p>\n",
    "\n",
    "<ul>\n",
    "<li> If $x_i$ is the $i^{\\text{th}}$ independent variable, then $\\nabla x_i = \\left<0,\\ldots,0,1,0,\\ldots, 0\\right>$, where the $1$ is in the $i^{\\text{th}}$ slot.  Another way to say this is the following.  Suppose my function $f$ just picked out the $i^{\\text{th}}$ variable: $f(x_1, \\ldots, x_p)=x_i$, for some $i$.  Then the gradient of $f$ would be length 1 and would point in the $x_i$-direction.</li>\n",
    "<li> The gradient is <strong>linear</strong>: </li><ul>\n",
    " <li> If $f(x_1, \\ldots, x_p) = w_1+w_2$, then $\\nabla f = \\nabla w_1 + \\nabla w_2$, and </li>\n",
    " <li> If $f(x_1, \\ldots, x_p) = c \\cdot w$, then $\\nabla f = c \\cdot \\nabla w$.</li>\n",
    " <li> (As an extension of the first point: if $f(x_1, \\ldots, x_p) = \\sum_{i=1}^n w_i$, then $\\nabla f = \\sum_{i=1}^n\\nabla w_i$.)</li>\n",
    " </ul>\n",
    "<li> The gradient has a <strong>power rule</strong>: </li>\n",
    " <ul>\n",
    " <li> If $f(x_1, \\ldots, x_p) = w^n$ then $\\nabla f = n w^{n-1} \\cdot \\nabla w$, </li>\n",
    " </ul>\n",
    "<li> The gradient has a <strong>product rule</strong>: </li>\n",
    " <ul>\n",
    " <li> If $f(x_1, \\ldots, x_p) = w_1\\cdot w_2$ then $\\nabla f = w_1 \\cdot \\nabla w_2 + w_2 \\cdot \\nabla w_1$,</li>\n",
    " </ul>\n",
    "    <li> The gradient does have a <em>difference rule</em> and a <em>quotient rule</em>, but you can just define yours using the facts that $w_1 - w_2 = w_1 + (-1) \\cdot w_2$ and $\\dfrac{w_1}{w_2} = w_1 \\cdot (w_2)^{-1}$, respectively.</li>\n",
    "<li> The gradient has a <strong>chain rule</strong>, which manifests itself in the following ways (the only ones we'll need):</li>\n",
    " <ul>\n",
    " <li> If $f(x_1, \\ldots, x_p) = e^w$ then $\\nabla f = e^w \\cdot \\nabla w$,</li>\n",
    " <li> If $f(x_1, \\ldots, x_p) = \\ln(w)$ then $\\nabla f = \\frac{1}{w} \\cdot \\nabla w$, </li>\n",
    " <li> There are lots of others, but we won't need them for now. </li>\n",
    " </ul>\n",
    "</ul>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning about the black box\n",
    "\n",
    "One of the most common conversation points written about machine learning, and artificial intelligence more generally, is the notion that machine learning algorithms are \"black boxes\". Roughly, this means that the programmer creates the code which _trains_ the algorithm, runs that training code, then has _no idea_ what's going on \"inside the box\" of their machine learning model. Another goal for you in this project is to gain a more nuanced understanding of this argument. I want you to do a little bit of research now, and then explore later, the following questions:\n",
    "\n",
    "* What does it mean to say that the creator of a machine learning model \"doesn't know what's going on inside their algorithm\"? What don't they know? _(hint: it has to do with parameters.)_\n",
    "* Do machine learning model creators truly have _no idea_ what happens in their models? Perhaps, by default they don't? Or, in practice, they don't? \n",
    "\n",
    "You can find whatever resources you want on this topic (and if you find a resource you especially like, _definitely_ share with us all!). Here are some articles for you to explore if you need some help.\n",
    "\n",
    "* [The dangers of trusting black-box machine learning](https://bdtechtalks.com/2020/07/27/black-box-ai-models/), by Ben Dickson\n",
    "* [Code-Dependent: Pros and Cons of the Algorithm Age](https://www.pewresearch.org/internet/2017/02/08/code-dependent-pros-and-cons-of-the-algorithm-age/)By Lee Rainie and Janna Anderson. This article is HUGE, and covers many conversations. The point we're looking for in here is the last one, \"Theme 7: The need grows for algorithmic literacy, transparency and oversight\".\n",
    "* [Machine Learning Algorithms are Not Black Boxes](https://towardsdatascience.com/machine-learning-algorithms-are-not-black-boxes-541ddaf760c3), by Zach Monge. This explores neural networks, which we haven't covered yet, but for now, just know that they are very similar to linear and logistic regression, just with thousands of parameters instead of just a handful ($m$ and $b$, or in a multidimensional version, $m_1, m_2, \\ldots, m_m$, and $b$)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"background-color: #EFDDFF; padding: 10px;\">\n",
    "    <p>Provide a reflection here about your findings.</p>\n",
    "    </div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_(Delete this text and add your reflection.)_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The `Variable` class\n",
    "\n",
    "Your main task in the project is to make a `Variable` class.  The `Variable` class is a node in the computational graph.  It should have the following attributes and methods:\n",
    "\n",
    "* an `__init__` method,\n",
    "* an `evaluate` method that allows you to evaluate the variable at a given value of the primitive (independent) variables,\n",
    "* a `grad` method that will compute return the gradient at a given value.  Here, you should use a numpy array. This might sound scary, but the main reason is so that you can compute with them nicely, later (see my implementation of a `grad` method below).  For example, to do this for $f(x_1, x_2, x_3) = x_1$, that `Variable`'s gradient function should `return np.array([1,0,0])`\n",
    "* appropriate methods/functionality to calculate the gradients and function evaluation for all the gradients defined above.\n",
    "* _(Optional)_ I found it helpful to have a class attribute called `all_variables` which is a list that keeps track of all the Variable objects in the order of creation.  There are fancy technical definitions and requirements of such a list, technically called a _Wengert list_, but for you it could just be a list that holds all your `Variable` objects (so in the constructor, you should append `self` to the list). You definitely may not need this, but some people benefit from making it. \n",
    " * If you do make this, you'll likely need a class method which resets the list to an empty list.  This allows you to redefine the expression each time you create a new computational graph. Google `@staticmethod` to see the notation.\n",
    "* _(old, replaced by dictionary version_ an attribute `inputs`, a list that records which inputs the current `Variable` objects takes.  This could highly leverage your wengert list.\n",
    "\n",
    "As an example, if I were to say:\n",
    "\n",
    "```\n",
    ">>> x = Variable()\n",
    ">>> y = Variable()\n",
    ">>> z = x + y\n",
    "```\n",
    "\n",
    "This will create a new `Variable` class for `z`, and initialize it appropriately, and give it the ability to take its gradient.  The way we can implement this is by defining a *magic* method called `__add__`.  Magic methods in Python are what give it such a great, readable API.  Here's an example `__add__` method, based on how I set up my constructor.  Yours might be a little bit different:\n",
    "\n",
    "```\n",
    "    def __add__(self, other):\n",
    "        \"\"\" Defines the functionality of the `+` operator. \"\"\"\n",
    "        if isinstance(other, (int, float)):\n",
    "            \"\"\" The derivative of a constant is zero. \"\"\"\n",
    "            return Variable(\n",
    "                                evaluate = lambda values: other + self.evaluate(values), \n",
    "                                grad = self.grad\n",
    "                            )\n",
    "        elif isinstance(other, Variable):\n",
    "            \"\"\" The differential operator is linear. \"\"\"\n",
    "            return Variable(\n",
    "                                evaluate = lambda values: self.evaluate(values) \n",
    "                                    + other.evaluate(values),\n",
    "                                grad = lambda values: self.grad(values) \n",
    "                                    + other.grad(values)\n",
    "                           )\n",
    "        else:\n",
    "            return NotImplemented\n",
    "```\n",
    "\n",
    "See [this link](http://www.diveintopython3.net/special-method-names.html#acts-like-number) to learn about the magic methods you will need to implement.\n",
    "\n",
    "The theory here is that you create the \"ugly\" `x.__add__(y)`-ish code once and only once, and then when you use the code later it's much more readable: `x + y`.  You'll need to do something like this for all the operations discussed above in the [gradients section](#-Rules-for-Gradients:).  The exception is for functions like `exp` and `log` (and if I wanted you to do other things, like `sin`, `cos`, `tan`, _etc._).  Those will need to be class methods.  They should still return a new `Variable` object.  See below for the API that they should support.\n",
    "\n",
    "Other methods you need to implement:\n",
    "\n",
    "* While you do need to have an `evaluate` method, if you also implement the `__call__` method (make it just call `evaluate`) then you can replace code of the form `w.evaluate({\"x_1\": 4, \"x_2\": 2, \"x_3\": 8.5})` with `w(x_1 = 4, x_2 = 2, x_3 = 8.5)`, which is both cleaner and looks like function notation (which is desirable because the `w`'s really can be thought of as functions of the independent variables).\n",
    "* You might want to implement the `__repr__` method so that you can print out your variables (as in, just `print(x)`).  It would be especially nice if printing out a `Variable` object gave you a graph traversal of its `inputs`.  It's okay here if it looks like a tree, _i.e._ nodes get repeated.  It would be a bit too much work to make the true graph visual happen.\n",
    "* You will need to implement the methods `__radd__` and `__rmul__`, _etc._ that give your `__add__` and `__mul__` method more functionality.  This allows expressions like `2 + x` instead of always requiring that a user use the `Variable` object first: `x + 2`.  In your research, you may see `__iadd__` and `__imul__`, and those you _should_ skip.\n",
    "\n",
    "Here are some resources that may help you along the way:\n",
    "* A good explanation on [Automatic Differentiation](https://pdfs.semanticscholar.org/be3d/17df872d41465dabda2fc9a9a61394658a1a.pdf) (the name of this process).  It's probably _much_ more information than you need.\n",
    "* There's always [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
    "* Another take on [operator overloading in Python](http://blog.teamtreehouse.com/operator-overloading-python)."
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A \"complete\" collection of uses that demonstrates the API \n",
    "Suppose my function is: \n",
    "\n",
    "$$f(x_1, x_2, x_3) = e^{x_1 + {x_2}^2} + 3 \\cdot \\ln(27 - x_1\\cdot x_2 \\cdot x_3).$$\n",
    "\n",
    "Then the following code would build and test my function. The first line might be different for you, depending on how you create the `exp` and `log` functions:\n",
    "\n",
    "```\n",
    ">>> from Variable import exp, log\n",
    ">>> x_1 = Variable(name = \"x_1\")\n",
    ">>> x_2 = Variable(name = \"x_2\")\n",
    ">>> x_3 = Variable(name = \"x_3\")\n",
    ">>> z = exp(x_1 + x_2**2) + 3 * log(27 - x_1 * x_2 * x_3)\n",
    ">>> # Evaluate the function at the point (x_1, x_2, x_3) = (3, 1, 7):\n",
    ">>> z(x_1 = 3, x_2 = 1, x_3 = 7)\n",
    "59.9734284408284\n",
    ">>> # Determine the gradient of the function at the point (x_1, x_2, x_3) = (3, 1, 7):\n",
    ">>> z.grad(x_1 = 3, x_2 = 1, x_3 = 7)\n",
    "array([ 51.09815003,  98.69630007,  -1.5       ])\n",
    ">>> print(x_1 + x_2**2)\n",
    "+(\n",
    " 1: <input 1>\n",
    " 2: ^2(\n",
    "  <input 2>\n",
    " )\n",
    ")\n",
    "```\n",
    "Two notes: \n",
    "1. I had to do some pencil-and-paper math on that gradient, let me know if it's not what you got!\n",
    "2. The print-out is optional, but helpful!  You can make it say whatever you want.  Here, I made it list the node type, then indent the inputs of that node by one, recursively.  It almost looks a bit like [prefix notation](https://en.wikipedia.org/wiki/Polish_notation).\n",
    "\n",
    "## Some thoughts on coding process\n",
    "\n",
    "* I recommend that you don't actually define the class in a single cell in this notebook.  It's going to get a bit big, and it's going to be difficult to work on it together if it's in a cell on a jupyter notebook.  I believe that a better strategy for designing this class is to have a Python file `variable.py`, located in the same folder as this notebook and opened in your favorite text editor that contains your `Variable` class, and then just rerun a cell like this to reimport it here after saving over there:"
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from variable import Variable"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Then, when you change something and want to check out how it works, just rerun the above cell.\n",
    "* This also makes sharing code via something like github a lot easier!\n",
    "* I _highly recommend_ you begin by making everything other than gradients work, and then add gradients afterward. It will be **much** easier to understand.\n",
    "* I also _highly recommend_ that you make it take in one-dimensional data at first (so that the parameters are just $m$ and $b$), and then allow for multidimensional inputs. For top marks on this assignment, one would need to add in multidimensional support, but it is a bit confusing to do so, and you will get almost complete understanding of the assignment by starting with one dimension."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Good luck!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The `LogisticRegression` class\n",
    "\n",
    "Now that you have the class needed for creating a gradient, it's time to put it to use.  Construct a `LogisticRegression` class that serves as a logistic regression model.  It should support the standard Scikit-Learn API:\n",
    "\n",
    "```\n",
    ">>> X,y = <some dataset consisting numpy arrays in the standard form>\n",
    ">>> model = LogisticRegression()\n",
    ">>> model.fit(X, y)\n",
    ">>> X_test, y_true = <some test dataset in the standard form>\n",
    ">>> y_preds = model.predict(X_test)\n",
    ">>> from sklearn.metrics import accuracy_score\n",
    ">>> accuracy_score(y_true, y_preds)\n",
    "```\n",
    "\n",
    "Use your `Variable` class in the `fit` method of your class (or wherever else seems appropriate).  Remember, you're taking the gradient of the cost function, and the cost function has as its inputs the parameters of your model.  So you should have a `Variable` for every model parameter.  You **don't** need to support multi-class predictions (as in, you can assume your output variable `y` is binary), and it is **optional** to support a multidimensional `X` (as in, perhaps my dataset has 7 predictors, and I want to be able to use your class to fit my data).\n",
    "\n",
    "You should fit your model to some data in order to (thoroughly!) test your model.\n",
    "\n",
    "**Note**: I recognize that you could just take the partial derivatives (or google them), but that's not the point of this project!  The goal is to struggle through both creating _and_ using your `Variable` class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Good luck!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploring the black box\n",
    "\n",
    "Now that you've got a `Variable` class that lets you build a model, and you've created a model using it, I want you to revisit the notion of a \"black box\" algorithm. Obviously, our model is \"small\" in terms of its number of parameters, but I want you to look into your code, explore your model parameters, and \"interpret\" your fit model. What if you had to defend, say in court, a decision made by your model? Could you fully interpret and explain your model's decision? How? (Hint, just as before, it's the parameters.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Good luck!"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('ml-venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "9e18352fe61ca699d23a1ad95306b1cb7f348970ceaca7c655580a6420f95203"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}